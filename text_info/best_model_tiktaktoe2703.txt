++++++++++++++++++++++
Agent_noOutActivation/20230327-183042 (Fabian PC)
++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
- 1 Block 3 layers 128 filters 3x3
- dense 64
- output default

Hyperparameters:
***************
# Hyperparameter
iterations = 10001 # only did 400
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512
#reward_function_adapting_agent = lambda d,r: tf.where(r==-0.1, tf.constant(0.1), tf.where(r==0.0,tf.constant(1.0),tf.where(r==1.0,tf.constant(-1.0), r)))
epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.998
POLYAK = 0.9
dropout_rate = 0
normalisation = True
BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 1 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False



++++++++++++++++++++++
Agent_TanH/20230327-192241 (Fabian PC)
++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
- 1 Block 3 layers 128 filters 3x3
- dense 64
- output tanh

Hyperparameters:
***************
# Hyperparameter
iterations = 10001
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512
#reward_function_adapting_agent = lambda d,r: tf.where(r==-0.1, tf.constant(0.1), tf.where(r==0.0,tf.constant(1.0),tf.where(r==1.0,tf.constant(-1.0), r)))
epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.998
POLYAK = 0.9
dropout_rate = 0
normalisation = True
BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 1 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False

++++++++++++++++++++++
agent_linear_decay0.99/20230327-185908 (Eosandra Laptop)
++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
- 1 Block 3 layers 128 filters 3x3
- dense 64
- output default

Hyperparameters:
***************
# Hyperparameter
iterations = 10001
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512
#reward_function_adapting_agent = lambda d,r: tf.where(r==-0.1, tf.constant(0.1), tf.where(r==0.0,tf.constant(1.0),tf.where(r==1.0,tf.constant(-1.0), r)))
epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.99
POLYAK = 0.9
dropout_rate = 0
normalisation = True
BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 1 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False
