++++++++++++++++++++++
Agent_noOutActivation/20230327-183042 (Fabian PC)
++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
- 1 Block 3 layers 128 filters 3x3
- dense 64
- output default

Hyperparameters:
***************
# Hyperparameter
iterations = 10001 # only did 400
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512
#reward_function_adapting_agent = lambda d,r: tf.where(r==-0.1, tf.constant(0.1), tf.where(r==0.0,tf.constant(1.0),tf.where(r==1.0,tf.constant(-1.0), r)))
epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.998
POLYAK = 0.9
dropout_rate = 0
normalisation = True
BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 1 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False



++++++++++++++++++++++
Agent_TanH/20230327-192241 (Fabian Notebook)
++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
- 1 Block 3 layers 128 filters 3x3
- dense 64
- output tanh

Hyperparameters:
***************
# Hyperparameter
iterations = 10001
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512
#reward_function_adapting_agent = lambda d,r: tf.where(r==-0.1, tf.constant(0.1), tf.where(r==0.0,tf.constant(1.0),tf.where(r==1.0,tf.constant(-1.0), r)))
epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.998
POLYAK = 0.9
dropout_rate = 0
normalisation = True
BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 1 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False

++++++++++++++++++++++
agent_linear_decay099/20230327-185908 (Eosandra Laptop)

max about 94% win
++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
- 1 Block 3 layers 128 filters 3x3
- dense 64
- output default

Hyperparameters:
***************
# Hyperparameter
iterations = 10001 # did 7859
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512
#reward_function_adapting_agent = lambda d,r: tf.where(r==-0.1, tf.constant(0.1), tf.where(r==0.0,tf.constant(1.0),tf.where(r==1.0,tf.constant(-1.0), r)))
epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.99
POLYAK = 0.9
dropout_rate = 0
normalisation = True
BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 1 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False


++++++++++++++++++++++
agent_ConnectFour_tanh/20230328-094318 (Fabian Notebook)

max reward 1.0 at iteration: 
400, 550, 900

++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
CONV_KERNEL = [4,4]
FILTERS = 128
HIDDEN_UNITS = [64,64]
loss = tf.keras.losses.MeanSquaredError()
output_activation = tf.nn.tanh


# Hyperparameter
#*****************
iterations = 10001 # only did 900
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512

epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.995
opponent_epsilon_function = lambda x: (x/2)

POLYAK = 0.9
dropout_rate = 0
normalisation = True

BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 1 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False

++++++++++++++++++++++
3agents_linear/20230328-21 (Eosandra Laptop)

max about 94% win
++++++++++++++++++++++

env: 
***
TikTakToeEnv, move_reward = 0

model architecture: 1 agent
******************
- 1 Block 3 layers 128 filters 3x3
- dense 64
- output default

Hyperparameters:
***************
# Hyperparameter
iterations = 10001 # did 
INNER_ITS = 50 *2
BATCH_SIZE = 256 #512

epsilon = 1
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.995
opponent_epsilon_function = lambda x: (x/4) if (x/4) > 0.05 else 0.05

POLYAK = 0.9
dropout_rate = 0
normalisation = True
BATCH_SIZE_SAMPLING = 512
SAMPLING = 2
AGENT_NUMBER = 3 # how many agents will play against each other while training
discount_factor_gamma = tf.constant(0.3)
unavailable_action_reward = False
D = 20
